deploy:
  phases:
    install:
      commands:
      - npm install -g aws-cdk@2.20.0
      - pip install -r requirements.txt
    build:
      commands:
      - aws iam create-service-linked-role --aws-service-name ecs-tasks.amazonaws.com || true
      # Login into ECR and create one if it doesnt exist
      - aws ecr describe-repositories --repository-names ${ECR_REPO_NAME} || aws ecr create-repository --repository-name ${ECR_REPO_NAME}
      - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
      - echo Building the Docker image...
      - cd image && docker build -t $REPOSITORY_URI:latest .
      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG
      - docker push $REPOSITORY_URI:latest && docker push $REPOSITORY_URI:$IMAGE_TAG
      - cdk deploy --require-approval never --progress events --app "python app.py" --outputs-file ./cdk-exports.json
      # Here we export some env vars and the write values into the sagemaker_dag_config.py file for use by the DAGs
      - export ADDF_MODULE_METADATA=$(python -c "import json; file=open('cdk-exports.json'); print(json.load(file)['addf-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}']['metadata'])")
      - export DAG_ROLE=$(echo ${ADDF_MODULE_METADATA} | jq -r ".DagRoleArn")
      - export ECR_REPO_NAME="addf-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}"
      - export COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - export IMAGE_TAG=${COMMIT_HASH:=latest}
      - export REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPO_NAME
      - echo "DEPLOYMENT_NAME = '${ADDF_DEPLOYMENT_NAME}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "MODULE_NAME = '${ADDF_MODULE_NAME}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "DAG_ROLE = '${DAG_ROLE}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "SIMULATION_MOCK_IMAGE = '${REPOSITORY_URI}:${IMAGE_TAG}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "REGION = '${AWS_DEFAULT_REGION}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "ACCOUNT_ID = '${AWS_ACCOUNT_ID}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "ON_DEMAND_JOB_QUEUE_ARN = '${ON_DEMAND_JOB_QUEUE_ARN}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "SPOT_JOB_QUEUE_ARN = '${SPOT_JOB_QUEUE_ARN}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      - echo "FARGATE_JOB_QUEUE_ARN = '${FARGATE_JOB_QUEUE_ARN}'" >> airflow_sagemaker_dags/sagemaker_dag_config.py
      # # Copy DAG files to S3
      - aws s3 cp --recursive airflow_sagemaker_dags/ s3://$ADDF_PARAMETER_DAG_BUCKET_NAME/$ADDF_PARAMETER_DAG_PATH/airflow_sagemaker_dags/
destroy:
  phases:
    install:
      commands:
      - npm install -g aws-cdk@2.20.0
      - pip install -r requirements.txt
    build:
      commands:
      # Remove DAG files
      - aws s3 rm --recursive s3://$ADDF_PARAMETER_DAG_BUCKET_NAME/$ADDF_PARAMETER_DAG_PATH/airflow_sagemaker_dags
      - cdk destroy --force --app "python app.py"